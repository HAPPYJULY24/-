"""
DataFetcher - The Smart Engine for Quant Data Bridge
Handles all data fetching, processing, and export logic.
"""

import pandas as pd
import yfinance as yf
import ccxt
from datetime import datetime, timedelta
import os
import pytz  # ğŸ†• æ—¶åŒºå¤„ç†


class DataFetcher:
    """
    Core class for fetching and processing financial data from multiple sources.
    """
    
    # Timeframe mapping for different APIs
    TIMEFRAME_MAP = {
        '1m': {'yf': '1m', 'ccxt': '1m'},
        '5m': {'yf': '5m', 'ccxt': '5m'},
        '15m': {'yf': '15m', 'ccxt': '15m'},
        '1h': {'yf': '1h', 'ccxt': '1h'},
        '1d': {'yf': '1d', 'ccxt': '1d'},
        '1w': {'yf': '1wk', 'ccxt': '1w'},   # æ–°å¢ï¼š1å‘¨
        '1M': {'yf': '1mo', 'ccxt': '1M'},   # æ–°å¢ï¼š1æœˆ
        '1y': {'yf': '1y', 'ccxt': '1y'},    # æ–°å¢ï¼š1å¹´
    }
    
    def __init__(self):
        self.last_error = None
        self.store_dir = "data/store"  # ğŸ†• Master DB ç›®å½•
    
    def preprocess_code(self, code: str, asset_type: str) -> str:
        """
        Preprocess asset code based on asset type.
        
        Args:
            code: Raw code from user input
            asset_type: Type of asset (Malaysia Stock, US Stock, Futures - Global, Crypto)
        
        Returns:
            Processed code ready for API call
        """
        code = code.strip()
        
        if asset_type == "Malaysia Stock":
            # If code is pure digits, append .KL suffix
            if code.isdigit():
                return f"{code}.KL"
            return code
        elif asset_type == "US Stock":
            return code
        elif asset_type == "Futures - Global":
            # ä¿®æ”¹ï¼šæœŸè´§ç°åœ¨ç›´æ¥é€ä¼ ç”¨æˆ·è¾“å…¥ï¼Œä¸å†å¼ºåˆ¶ GC=F
            return code  # ç”¨æˆ·å¯ä»¥è¾“å…¥ GC=F, CL=F, SI=F, ES=F ç­‰ä»»ä½•æœŸè´§ä»£ç 
        elif asset_type == "Crypto":
            return code
        
        return code
    
    def fetch_data(self, asset_type: str, code: str, timeframe: str, 
                   start_date: datetime, end_date: datetime,
                   exchange: str = None, proxy_url: str = None,
                   filter_lunch: bool = False) -> pd.DataFrame:  # ğŸ†• v2.0: åˆä¼‘è¿‡æ»¤å¼€å…³
        """
        Main data fetching router.
        
        Args:
            asset_type: Type of asset
            code: Asset code (already preprocessed)
            timeframe: Time granularity (1m, 5m, 15m, 1h, 1d)
            start_date: Start date for data
            end_date: End date for data
            exchange: Exchange name for crypto (e.g., "Luno (Malaysia)")  # æ–°å¢
            proxy_url: Proxy URL if enabled (e.g., "http://127.0.0.1:7890")  # æ–°å¢
        
        Returns:
            DataFrame with fetched data
        
        Raises:
            Exception: If data fetching fails
        """
        self.last_error = None
        
        try:
            # ä¿®æ”¹ï¼šFutures - Global ä¸è‚¡ç¥¨ä½¿ç”¨ç›¸åŒçš„ yfinance è·¯å¾„
            if asset_type in ["Malaysia Stock", "US Stock", "Futures - Global"]:
                df = self._fetch_stock_futures(code, timeframe, start_date, end_date)
            elif asset_type == "Crypto":
                # ä¼ é€’äº¤æ˜“æ‰€å’Œä»£ç†å‚æ•°ï¼ˆæ–°å¢ï¼‰
                df = self._fetch_crypto(code, timeframe, start_date, end_date,
                                       exchange=exchange, proxy_url=proxy_url)
            else:
                raise ValueError(f"Unknown asset type: {asset_type}")
            
            if df is None or df.empty:
                raise ValueError(f"No data found for {code}")
            
            # Standardize the dataframe
            df = self.standardize_dataframe(df)
            
            # ğŸ†• v2.0: æ—¶åŒºæ ‡å‡†åŒ–ï¼ˆå¼ºåˆ¶å¯ç”¨ï¼‰
            df = self._standardize_timezone(df)
            
            # ğŸ†• v2.0: åˆä¼‘è¿‡æ»¤ï¼ˆå¯é€‰ï¼Œç”± UI æ§åˆ¶ï¼‰
            if filter_lunch:
                df = self._filter_lunch_break(df, asset_type)
            
            return df
        
        except Exception as e:
            self.last_error = str(e)
            raise
    
    def _fetch_stock_futures(self, code: str, timeframe: str, 
                            start_date: datetime, end_date: datetime) -> pd.DataFrame:
        """
        Fetch data using yfinance for stocks and futures.
        
        Args:
            code: Asset code
            timeframe: Time granularity
            start_date: Start date
            end_date: End date
        
        Returns:
            DataFrame with OHLCV data
        
        Raises:
            Exception: If yfinance API fails (including minute-level restrictions)
        """
        try:
            print(f"[DEBUG] yfinance: Creating ticker for {code}")
            ticker = yf.Ticker(code)
            interval = self.TIMEFRAME_MAP[timeframe]['yf']
            
            print(f"[DEBUG] yfinance: Fetching data with interval={interval}, start={start_date}, end={end_date}")
            # yfinance download
            df = ticker.history(
                start=start_date,
                end=end_date,
                interval=interval,
                auto_adjust=False
            )
            
            print(f"[DEBUG] yfinance: Received data type: {type(df)}")
            
            # æ£€æŸ¥è¿”å›å€¼æ˜¯å¦ä¸º Noneï¼ˆæ–­ç½‘æ—¶å¯èƒ½å‘ç”Ÿï¼‰
            if df is None:
                raise Exception(
                    f"ç½‘ç»œè¿æ¥å¤±è´¥ï¼\n\n"
                    f"âš ï¸ æ— æ³•è¿æ¥åˆ°æ•°æ®æºæœåŠ¡å™¨ã€‚\n\n"
                    f"å¯èƒ½åŸå› ï¼š\n"
                    f"1. æ‚¨çš„ç”µè„‘æœªè¿æ¥åˆ°äº’è”ç½‘\n"
                    f"2. é˜²ç«å¢™é˜»æ­¢äº†ç¨‹åºè®¿é—®ç½‘ç»œ\n"
                    f"3. æ•°æ®æºæœåŠ¡å™¨æš‚æ—¶æ— æ³•è®¿é—®\n\n"
                    f"å»ºè®®ï¼š\n"
                    f"- æ£€æŸ¥æ‚¨çš„ç½‘ç»œè¿æ¥\n"
                    f"- ç¡®è®¤å¯ä»¥è®¿é—®äº’è”ç½‘\n"
                    f"- ç¨åé‡è¯•"
                )
            
            print(f"[DEBUG] yfinance: Received {len(df)} rows")
            
            if df.empty:
                raise ValueError(f"No data returned from yfinance for {code}. "
                               f"Asset may not exist or date range may be invalid.")
            
            # Reset index to make Date a column
            print("[DEBUG] yfinance: Resetting index...")
            df.reset_index(inplace=True)
            print(f"[DEBUG] yfinance: DataFrame columns: {list(df.columns)}")
            print(f"[DEBUG] yfinance: First row: {df.iloc[0].to_dict() if len(df) > 0 else 'N/A'}")
            
            return df
        
        except Exception as e:
            # Catch yfinance errors including minute-level data restrictions
            error_msg = str(e)
            error_type = type(e).__name__
            print(f"[DEBUG] yfinance ERROR: {error_msg}")
            print(f"[DEBUG] Error type: {error_type}")
            
            # æ£€æµ‹ TypeError with NoneTypeï¼ˆyfinance å†…éƒ¨æ–­ç½‘æ—¶æŠ›å‡ºï¼‰
            if error_type == "TypeError" and "NoneType" in error_msg:
                raise Exception(
                    f"ç½‘ç»œè¿æ¥å¤±è´¥ï¼\n\n"
                    f"âš ï¸ æ— æ³•è¿æ¥åˆ°æ•°æ®æºæœåŠ¡å™¨ã€‚\n\n"
                    f"å¯èƒ½åŸå› ï¼š\n"
                    f"1. æ‚¨çš„ç”µè„‘æœªè¿æ¥åˆ°äº’è”ç½‘\n"
                    f"2. é˜²ç«å¢™é˜»æ­¢äº†ç¨‹åºè®¿é—®ç½‘ç»œ\n"
                    f"3. æ•°æ®æºæœåŠ¡å™¨æš‚æ—¶æ— æ³•è®¿é—®\n\n"
                    f"å»ºè®®ï¼š\n"
                    f"- æ£€æŸ¥æ‚¨çš„ç½‘ç»œè¿æ¥\n"
                    f"- ç¡®è®¤å¯ä»¥è®¿é—®äº’è”ç½‘\n"
                    f"- æ£€æŸ¥é˜²ç«å¢™è®¾ç½®\n"
                    f"- ç¨åé‡è¯•"
                )
            
            # æ£€æµ‹å…¶ä»–ç½‘ç»œè¿æ¥é”™è¯¯
            network_error_keywords = [
                'connection', 'timeout', 'network', 'unreachable',
                'failed to establish', 'timed out', 'refused',
                'no internet', 'dns', 'resolve', 'gaierror',
                'ConnectionError', 'TimeoutError', 'URLError'
            ]
            
            is_network_error = any(keyword.lower() in error_msg.lower() or keyword.lower() in error_type.lower() 
                                  for keyword in network_error_keywords)
            
            if is_network_error:
                raise Exception(
                    f"ç½‘ç»œè¿æ¥å¤±è´¥ï¼\n\n"
                    f"âš ï¸ æ— æ³•è¿æ¥åˆ°æ•°æ®æºæœåŠ¡å™¨ã€‚\n\n"
                    f"å¯èƒ½åŸå› ï¼š\n"
                    f"1. æ‚¨çš„ç”µè„‘æœªè¿æ¥åˆ°äº’è”ç½‘\n"
                    f"2. é˜²ç«å¢™é˜»æ­¢äº†ç¨‹åºè®¿é—®ç½‘ç»œ\n"
                    f"3. æ•°æ®æºæœåŠ¡å™¨æš‚æ—¶æ— æ³•è®¿é—®\n\n"
                    f"å»ºè®®ï¼š\n"
                    f"- æ£€æŸ¥æ‚¨çš„ç½‘ç»œè¿æ¥\n"
                    f"- ç¡®è®¤å¯ä»¥è®¿é—®äº’è”ç½‘\n"
                    f"- æ£€æŸ¥é˜²ç«å¢™è®¾ç½®\n"
                    f"- ç¨åé‡è¯•"
                )
            
            # ç¿»è¯‘å¸¸è§çš„ yfinance é”™è¯¯ä¸ºç”¨æˆ·å‹å¥½çš„ä¸­æ–‡æç¤º
            elif "No data found" in error_msg or "No data returned" in error_msg:
                raise Exception(
                    f"æ‰¾ä¸åˆ°æ•°æ®ï¼\n\n"
                    f"å¯èƒ½åŸå› ï¼š\n"
                    f"1. è‚¡ç¥¨ä»£ç  '{code}' ä¸å­˜åœ¨\n"
                    f"2. è¯¥è‚¡ç¥¨åœ¨é€‰å®šçš„æ—¥æœŸèŒƒå›´å†…åœç‰Œ\n"
                    f"3. æ•°æ®æºæš‚æ—¶æ— æ³•è®¿é—®\n\n"
                    f"å»ºè®®ï¼š\n"
                    f"- æ£€æŸ¥è‚¡ç¥¨ä»£ç æ˜¯å¦æ­£ç¡®\n"
                    f"- å°è¯•ç¼©çŸ­æ—¥æœŸèŒƒå›´\n"
                    f"- ç¨åå†è¯•"
                )
            elif "1m data not available" in error_msg or "minute" in error_msg.lower():
                raise Exception(
                    f"åˆ†é’Ÿçº§æ•°æ®é™åˆ¶ï¼\n\n"
                    f"yfinance ä»…æä¾›æœ€è¿‘ 7-30 å¤©çš„åˆ†é’Ÿçº§æ•°æ®ã€‚\n\n"
                    f"å»ºè®®ï¼š\n"
                    f"- ç¼©çŸ­æ—¥æœŸèŒƒå›´ï¼ˆé€‰æ‹©æœ€è¿‘1ä¸ªæœˆå†…ï¼‰\n"
                    f"- æˆ–è€…é€‰æ‹© '1d' æ—¶é—´ç²’åº¦è·å–æ—¥çº¿æ•°æ®"
                )
            elif "Asset may not exist" in error_msg:
                raise Exception(
                    f"èµ„äº§ä¸å­˜åœ¨ï¼\n\n"
                    f"è‚¡ç¥¨ä»£ç  '{code}' å¯èƒ½ä¸æ­£ç¡®ã€‚\n\n"
                    f"ç¤ºä¾‹ï¼š\n"
                    f"- é©¬è‚¡ï¼š1155ï¼ˆä¼šè‡ªåŠ¨æ·»åŠ .KLåç¼€ï¼‰\n"
                    f"- ç¾è‚¡ï¼šAAPL, TSLA, MSFT"
                )
            else:
                # å…¶ä»–æœªçŸ¥é”™è¯¯ï¼Œæ˜¾ç¤ºåŸå§‹é”™è¯¯ä¿¡æ¯
                raise Exception(f"æ•°æ®è·å–å¤±è´¥ï¼š{error_msg}")
    
    
    def _fetch_crypto(self, pair: str, timeframe: str, 
                     start_date: datetime, end_date: datetime,
                     exchange: str = None, proxy_url: str = None) -> pd.DataFrame:
        """
        Fetch crypto data from selected exchange with optional proxy.
        
        Args:
            pair: Trading pair (e.g., BTC/USDT)
            timeframe: Time granularity
            start_date: Start date
            end_date: End date
            exchange: Exchange name (e.g., "Luno (Malaysia)", "Binance (Global)")
            proxy_url: Proxy URL if enabled
        
        Returns:
            DataFrame with OHLCV data
        """
        # äº¤æ˜“æ‰€æ˜ å°„
        EXCHANGE_MAP = {
            "Luno (Malaysia)": ccxt.luno,
            "Binance (Global)": ccxt.binance,
            "OKX": ccxt.okx,
            "Bybit": ccxt.bybit
        }
        
        # é»˜è®¤ä½¿ç”¨ Luno
        if not exchange:
            exchange = "Luno (Malaysia)"
        
        print(f"[DEBUG] Crypto: Using exchange: {exchange}")
        print(f"[DEBUG] Crypto: Proxy enabled: {proxy_url is not None}")
        
        # è·å–äº¤æ˜“æ‰€ç±»
        exchange_class = EXCHANGE_MAP.get(exchange)
        if not exchange_class:
            raise Exception(f"ä¸æ”¯æŒçš„äº¤æ˜“æ‰€: {exchange}")
        
        # é…ç½®äº¤æ˜“æ‰€ï¼ˆåŒ…æ‹¬ä»£ç†ï¼‰
        config = {}
        if proxy_url:
            config['proxies'] = {
                'http': proxy_url,
                'https': proxy_url
            }
            print(f"[DEBUG] Crypto: Proxy configured: {proxy_url}")
        
        try:
            # åˆ›å»ºäº¤æ˜“æ‰€å®ä¾‹
            exchange_instance = exchange_class(config)
            exchange_instance.load_markets()
            
            # è·å– ccxt æ—¶é—´ç²’åº¦
            ccxt_timeframe = self.TIMEFRAME_MAP[timeframe]['ccxt']
            
            # è½¬æ¢å¼€å§‹æ—¶é—´ä¸ºæ—¶é—´æˆ³
            since = int(start_date.timestamp() * 1000)
            
            print(f"[DEBUG] {exchange}: Fetching {pair} with timeframe {ccxt_timeframe}")
            
            # è·å– OHLCV æ•°æ®
            ohlcv = exchange_instance.fetch_ohlcv(
                symbol=pair,
                timeframe=ccxt_timeframe,
                since=since
            )
            
            if not ohlcv:
                raise Exception(f"ä» {exchange} è·å–ä¸åˆ°æ•°æ®")
            
            # è½¬æ¢ä¸º DataFrame
            df = pd.DataFrame(
                ohlcv,
                columns=['timestamp', 'Open', 'High', 'Low', 'Close', 'Volume']
            )
            
            # è½¬æ¢æ—¶é—´æˆ³ä¸ºæ—¥æœŸæ—¶é—´
            df['Date'] = pd.to_datetime(df['timestamp'], unit='ms')
            df.drop('timestamp', axis=1, inplace=True)
            
            # æŒ‰ç»“æŸæ—¥æœŸè¿‡æ»¤
            df = df[df['Date'] <= end_date]
            
            if df.empty:
                raise Exception(f"æŒ‡å®šæ—¥æœŸèŒƒå›´å†…æ²¡æœ‰æ•°æ®")
            
            print(f"[DEBUG] {exchange}: Got {len(df)} rows")
            return df
            
        except Exception as e:
            error_msg = str(e)
            error_type = type(e).__name__
            print(f"[DEBUG] {exchange} ERROR: {error_msg}")
            print(f"[DEBUG] Error type: {error_type}")
            
            # æ£€æµ‹ç½‘ç»œè¿æ¥é”™è¯¯
            network_error_keywords = [
                'connection', 'timeout', 'network', 'unreachable',
                'failed to establish', 'timed out', 'refused',
                'no internet', 'dns', 'resolve', 'gaierror',
                'ConnectionError', 'TimeoutError', 'URLError',
                'RequestException', 'ConnectTimeout'
            ]
            
            is_network_error = any(keyword.lower() in error_msg.lower() or keyword.lower() in error_type.lower() 
                                  for keyword in network_error_keywords)
            
            if is_network_error:
                proxy_tip = "\n\nğŸ’¡ æç¤ºï¼šå¦‚æœäº¤æ˜“æ‰€è¢«å¢™ï¼Œè¯·å°è¯•å¯ç”¨ä»£ç†è®¾ç½®ã€‚" if not proxy_url else ""
                raise Exception(
                    f"ç½‘ç»œè¿æ¥å¤±è´¥ï¼\n\n"
                    f"âš ï¸ æ— æ³•è¿æ¥åˆ° {exchange}ã€‚\n\n"
                    f"å¯èƒ½åŸå› ï¼š\n"
                    f"1. äº¤æ˜“æ‰€è¢«é˜²ç«å¢™å±è”½\n"
                    f"2. ç½‘ç»œè¿æ¥é—®é¢˜\n"
                    f"3. ä»£ç†é…ç½®é”™è¯¯ï¼ˆå¦‚æœå·²å¯ç”¨ï¼‰\n\n"
                    f"å»ºè®®ï¼š\n"
                    f"- æ£€æŸ¥ç½‘ç»œè¿æ¥\n"
                    f"- å°è¯•åˆ‡æ¢åˆ°å…¶ä»–äº¤æ˜“æ‰€\n"
                    f"- å¯ç”¨æˆ–æ£€æŸ¥ä»£ç†è®¾ç½®{proxy_tip}"
                )
            else:
                raise Exception(f"{exchange} æ•°æ®è·å–å¤±è´¥ï¼š{error_msg}")
    
    def standardize_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Standardize DataFrame columns and format.
        
        Args:
            df: Raw DataFrame from API
        
        Returns:
            Standardized DataFrame with columns: Date, Open, High, Low, Close, Volume
        """
        print(f"[DEBUG] Standardizing DataFrame with columns: {list(df.columns)}")
        
        # First, drop extra columns we don't need (like Adj Close, Dividends, Stock Splits)
        # Keep only the columns we want to map
        columns_to_drop = []
        for col in df.columns:
            col_lower = col.lower()
            # Drop Adj Close, Dividends, Stock Splits, etc.
            if 'adj' in col_lower or 'dividend' in col_lower or 'split' in col_lower:
                columns_to_drop.append(col)
        
        if columns_to_drop:
            print(f"[DEBUG] Dropping extra columns: {columns_to_drop}")
            df = df.drop(columns=columns_to_drop)
        
        # Rename columns to standard format
        column_mapping = {}
        
        for col in df.columns:
            col_lower = col.lower()
            if 'date' in col_lower or 'time' in col_lower or col == 'Date':
                column_mapping[col] = 'Date'
            elif 'open' in col_lower:
                column_mapping[col] = 'Open'
            elif 'high' in col_lower:
                column_mapping[col] = 'High'
            elif 'low' in col_lower:
                column_mapping[col] = 'Low'
            elif 'close' in col_lower:
                column_mapping[col] = 'Close'
            elif 'volume' in col_lower or 'vol' in col_lower:
                column_mapping[col] = 'Volume'
        
        print(f"[DEBUG] Column mapping: {column_mapping}")
        df = df.rename(columns=column_mapping)
        
        # Keep only required columns (handle case where column might not exist)
        required_cols = ['Date', 'Open', 'High', 'Low', 'Close', 'Volume']
        existing_cols = [col for col in required_cols if col in df.columns]
        
        print(f"[DEBUG] Keeping columns: {existing_cols}")
        df = df[existing_cols]
        
        # Verify no duplicate columns
        if len(df.columns) != len(set(df.columns)):
            duplicates = [col for col in df.columns if list(df.columns).count(col) > 1]
            print(f"[DEBUG] WARNING: Duplicate columns found: {set(duplicates)}")
            # Remove duplicates by keeping only the first occurrence
            df = df.loc[:, ~df.columns.duplicated()]
            print(f"[DEBUG] After removing duplicates, columns: {list(df.columns)}")
        
        # Convert Date to string format: YYYY-MM-DD HH:MM:SS
        if 'Date' in df.columns:
            print("[DEBUG] Converting Date column to string format...")
            df['Date'] = pd.to_datetime(df['Date']).dt.strftime('%Y-%m-%d %H:%M:%S')
        
        print(f"[DEBUG] Standardization complete. Final shape: {df.shape}, Columns: {list(df.columns)}")
        return df
    
    def _standardize_timezone(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        æ ‡å‡†åŒ–æ—¶åŒºä¸º Asia/Kuala_Lumpur (v2.0)
        
        Args:
            df: åŸå§‹DataFrameï¼ŒDateåˆ—å¯èƒ½ä¸ºUTCæˆ–æ— æ—¶åŒº
        
        Returns:
            æ—¶åŒºæ ‡å‡†åŒ–åçš„DataFrame
        """
        print("[DEBUG] Standardizing timezone to Asia/Kuala_Lumpur...")
        
        KL_TZ = pytz.timezone('Asia/Kuala_Lumpur')
        
        # ç¡®ä¿Dateåˆ—ä¸ºdatetimeç±»å‹
        df['Date'] = pd.to_datetime(df['Date'])
        
        # å¦‚æœæ²¡æœ‰æ—¶åŒºä¿¡æ¯ï¼Œå‡å®šä¸ºUTC
        if df['Date'].dt.tz is None:
            print("[DEBUG] No timezone info, assuming UTC")
            df['Date'] = df['Date'].dt.tz_localize('UTC')
        
        # è½¬æ¢ä¸ºå‰éš†å¡æ—¶åŒº
        df['Date'] = df['Date'].dt.tz_convert(KL_TZ)
        print(f"[DEBUG] Timezone converted. Sample: {df['Date'].iloc[0]}")
        
        # ç§»é™¤æ—¶åŒºä¿¡æ¯ï¼Œä¿ç•™æœ¬åœ°æ—¶é—´ï¼ˆé¿å…Parquetå…¼å®¹æ€§é—®é¢˜ï¼‰
        df['Date'] = df['Date'].dt.tz_localize(None)
        
        # ğŸ”§ FIX: è½¬æ¢å›å­—ç¬¦ä¸²æ ¼å¼ï¼Œç¡®ä¿ä¸ analyze_gaps() å…¼å®¹
        df['Date'] = df['Date'].dt.strftime('%Y-%m-%d %H:%M:%S')
        
        print("[DEBUG] Timezone standardization complete")
        return df
    
    def _filter_lunch_break(self, df: pd.DataFrame, asset_type: str) -> pd.DataFrame:
        """
        è¿‡æ»¤åˆä¼‘æ—¶æ®µ (12:30-14:30 MYT) - é»‘åå•ç­–ç•¥ (v2.0)
        
        ç­–ç•¥ï¼šå‰”é™¤åˆä¼‘å™ªéŸ³ï¼Œä¿ç•™æ‰€æœ‰å…¶ä»–æ—¶é—´ï¼ˆåŒ…æ‹¬ç›˜å‰ç›˜åï¼‰
        é€‚ç”¨äºï¼šMalaysia Stock + FKLI/FCPO
        
        Args:
            df: åŸå§‹DataFrame
            asset_type: èµ„äº§ç±»å‹
        
        Returns:
            è¿‡æ»¤åçš„DataFrame
        """
        # åªå¯¹é©¬è‚¡èµ„äº§è¿‡æ»¤
        if asset_type not in ["Malaysia Stock", "Futures - Global"]:
            print(f"[DEBUG] Skipping lunch filter for {asset_type}")
            return df
        
        print(f"[DEBUG] Applying lunch break filter for {asset_type}")
        
        # ç¡®ä¿Dateåˆ—ä¸ºdatetime
        df['Date'] = pd.to_datetime(df['Date'])
        
        # æå–å°æ—¶å’Œåˆ†é’Ÿ
        df['_hour'] = df['Date'].dt.hour
        df['_minute'] = df['Date'].dt.minute
        
        # ğŸ”¥ é»‘åå•ç­–ç•¥ï¼šå®šä¹‰åˆä¼‘æ—¶æ®µï¼ˆè¦è¢«å‰”é™¤çš„ï¼‰
        is_lunch_break = (
            (df['_hour'] == 12) & (df['_minute'] > 30)  # 12:31 - 12:59
        ) | (
            (df['_hour'] == 13)                          # 13:00 - 13:59
        ) | (
            (df['_hour'] == 14) & (df['_minute'] < 30)  # 14:00 - 14:29
        )
        
        # è¿‡æ»¤ï¼šä¿ç•™æ‰€æœ‰éåˆä¼‘æ—¶æ®µçš„æ•°æ®
        filtered_df = df[~is_lunch_break].copy()  # ğŸ¯ æ³¨æ„è¿™é‡Œæ˜¯ ~ï¼ˆå–åï¼‰
        
        # åˆ é™¤ä¸´æ—¶åˆ—
        filtered_df.drop(['_hour', '_minute'], axis=1, inplace=True)
        
        # ğŸ”§ FIX: è½¬æ¢å›å­—ç¬¦ä¸²æ ¼å¼ï¼Œç¡®ä¿ä¸åç»­æ–¹æ³•å…¼å®¹
        filtered_df['Date'] = filtered_df['Date'].dt.strftime('%Y-%m-%d %H:%M:%S')
        
        removed_count = len(df) - len(filtered_df)
        print(f"[DEBUG] Filtered {removed_count} lunch break records")
        
        return filtered_df
    
    def smart_update(self, symbol: str, asset_type: str, timeframe: str,
                     start_date: datetime = None, end_date: datetime = None,
                     exchange: str = None, proxy_url: str = None) -> pd.DataFrame:
        """
        æ™ºèƒ½å¢é‡æ›´æ–°ç­–ç•¥ (v2.0 - Master DB)
        
        å·¥ä½œæµç¨‹ï¼š
        1. æ£€æŸ¥ data/store/{symbol}_{timeframe}.parquet æ˜¯å¦å­˜åœ¨
        2. å¦‚æœå­˜åœ¨ï¼Œè¯»å–æœ€åä¸€æ¡è®°å½•çš„æ—¶é—´æˆ³
        3. ä¸‹è½½ last_date+1 åˆ° end_date çš„æ–°æ•°æ®
        4. åˆå¹¶å»é‡ï¼Œè¦†ç›–ä¿å­˜åˆ° Master DB
        5. å¦‚æœä¸å­˜åœ¨ï¼Œæ‰§è¡Œå…¨é‡ä¸‹è½½
        
        Args:
            symbol: èµ„äº§ä»£ç ï¼ˆå·²é¢„å¤„ç†ï¼‰
            asset_type: èµ„äº§ç±»å‹
            timeframe: æ—¶é—´ç²’åº¦
            start_date: å¼€å§‹æ—¥æœŸï¼ˆä»…ç”¨äºå…¨é‡ä¸‹è½½ï¼‰
            end_date: ç»“æŸæ—¥æœŸï¼ˆé»˜è®¤ä¸ºä»Šå¤©ï¼‰
            exchange: äº¤æ˜“æ‰€ï¼ˆåŠ å¯†è´§å¸ï¼‰
            proxy_url: ä»£ç†URL
        
        Returns:
            åˆå¹¶åçš„å®Œæ•´DataFrame
        """
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(self.store_dir, exist_ok=True)
        
        # ç”Ÿæˆ Master DB æ–‡ä»¶åï¼ˆå›ºå®šï¼Œä¸å¸¦æ—¥æœŸï¼‰
        filename = f"{symbol}_{timeframe}.parquet"
        filepath = os.path.join(self.store_dir, filename)
        
        # é»˜è®¤ç»“æŸæ—¥æœŸä¸ºä»Šå¤©
        if end_date is None:
            end_date = datetime.now()
        
        # æ£€æŸ¥æœ¬åœ° Master DB æ˜¯å¦å­˜åœ¨
        if os.path.exists(filepath):
            print(f"[DEBUG] Found Master DB: {filepath}")
            
            try:
                # è¯»å–ç°æœ‰æ•°æ®
                existing_df = pd.read_parquet(filepath)
                
                # è·å–æœ€åä¸€æ¡è®°å½•çš„æ—¥æœŸ
                existing_df['Date'] = pd.to_datetime(existing_df['Date'])
                last_date = existing_df['Date'].max()
                
                print(f"[DEBUG] Last record date: {last_date}")
                print(f"[DEBUG] Existing records: {len(existing_df)}")
                
                # ä¸‹è½½å¢é‡æ•°æ® (last_date+1 åˆ° end_date)
                incremental_start = last_date + timedelta(days=1)
                
                # å¦‚æœå¢é‡å¼€å§‹æ—¶é—´å·²ç»è¶…è¿‡ç»“æŸæ—¶é—´ï¼Œè¯´æ˜æ²¡æœ‰æ–°æ•°æ®
                if incremental_start > end_date:
                    print("[DEBUG] No new data needed, returning existing Master DB")
                    # ğŸ”§ FIX: è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼å†è¿”å›
                    existing_df['Date'] = existing_df['Date'].dt.strftime('%Y-%m-%d %H:%M:%S')
                    return existing_df
                
                print(f"[DEBUG] Incremental download: {incremental_start} to {end_date}")
                
                # è·å–å¢é‡æ•°æ®ï¼ˆè°ƒç”¨åŸæœ‰çš„ fetch_dataï¼‰
                new_df = self.fetch_data(
                    asset_type=asset_type,
                    code=symbol,
                    timeframe=timeframe,
                    start_date=incremental_start,
                    end_date=end_date,
                    exchange=exchange,
                    proxy_url=proxy_url
                )
                
                if new_df.empty:
                    print("[DEBUG] No new data fetched, returning existing Master DB")
                    # ğŸ”§ FIX: è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼å†è¿”å›
                    existing_df['Date'] = existing_df['Date'].dt.strftime('%Y-%m-%d %H:%M:%S')
                    return existing_df
                
                print(f"[DEBUG] Fetched {len(new_df)} new records")
                
                # åˆå¹¶æ•°æ®
                combined_df = pd.concat([existing_df, new_df], ignore_index=True)
                
                # å»é‡ï¼ˆä¿ç•™æœ€æ–°ï¼‰
                combined_df['Date'] = pd.to_datetime(combined_df['Date'])
                combined_df = combined_df.drop_duplicates(subset=['Date'], keep='last')
                combined_df = combined_df.sort_values('Date').reset_index(drop=True)
                
                # ğŸ”§ FIX: è½¬æ¢å›å­—ç¬¦ä¸²æ ¼å¼
                combined_df['Date'] = combined_df['Date'].dt.strftime('%Y-%m-%d %H:%M:%S')
                
                print(f"[DEBUG] After merge and dedup: {len(combined_df)} total records")
                
            except Exception as e:
                print(f"[DEBUG] Error reading Master DB: {str(e)}")
                print("[DEBUG] Falling back to full download")
                
                # å¦‚æœè¯»å–å¤±è´¥ï¼Œæ‰§è¡Œå…¨é‡ä¸‹è½½
                if start_date is None:
                    start_date = end_date - timedelta(days=365)  # é»˜è®¤1å¹´
                
                combined_df = self.fetch_data(
                    asset_type=asset_type,
                    code=symbol,
                    timeframe=timeframe,
                    start_date=start_date,
                    end_date=end_date,
                    exchange=exchange,
                    proxy_url=proxy_url
                )
        
        else:
            print(f"[DEBUG] No Master DB found, executing full download")
            
            # é¦–æ¬¡ä¸‹è½½ï¼šå…¨é‡
            if start_date is None:
                start_date = end_date - timedelta(days=365)  # é»˜è®¤1å¹´
            
            combined_df = self.fetch_data(
                asset_type=asset_type,
                code=symbol,
                timeframe=timeframe,
                start_date=start_date,
                end_date=end_date,
                exchange=exchange,
                proxy_url=proxy_url
            )
        
        # ä¿å­˜åˆ° Master DBï¼ˆè¦†ç›–ï¼‰
        combined_df.to_parquet(filepath, index=False, compression='snappy')
        print(f"[DEBUG] Master DB updated: {filepath}")
        
        return combined_df
    

    def analyze_gaps(self, df: pd.DataFrame, requested_start: datetime, 
                     requested_end: datetime) -> tuple[bool, str]:
        """
        Analyze data gaps and determine if warning is needed.
        
        Args:
            df: Fetched DataFrame
            requested_start: User-requested start date
            requested_end: User-requested end date
        
        Returns:
            Tuple of (has_warning, warning_message)
            - has_warning: True if gap > 3 days
            - warning_message: Warning text to display
        """
        if df.empty:
            return True, "æ•°æ®ä¸ºç©º"
        
        # Get actual start date (first row)
        first_date_str = df.iloc[0]['Date']
        actual_start = datetime.strptime(first_date_str, '%Y-%m-%d %H:%M:%S')
        
        # Calculate difference
        diff = actual_start - requested_start
        
        # 3-day tolerance
        if diff.days > 3:
            warning_msg = f"è­¦å‘Šï¼šæ•°æ®ä¸å®Œæ•´ã€‚æºæ•°æ®å¼€å§‹äº {first_date_str}ï¼Œè¯·æ±‚å¼€å§‹äº {requested_start.strftime('%Y-%m-%d')}"
            return True, warning_msg
        
        return False, "æ•°æ®è·å–æˆåŠŸï¼è¦†ç›–ç‡ 100%"
    
    def export_to_csv(self, df: pd.DataFrame, code: str, timeframe: str, 
                      start_date: datetime) -> str:
        """
        Export DataFrame to CSV file.
        
        Args:
            df: DataFrame to export
            code: Asset code
            timeframe: Timeframe used
            start_date: Start date used
        
        Returns:
            Path to saved CSV file
        """
        # Create filename: {Code}_{Timeframe}_{StartDate}.csv
        start_str = start_date.strftime('%Y%m%d')
        filename = f"{code}_{timeframe}_{start_str}.csv"
        
        # Save to current directory or a data folder
        output_dir = "exported_data"
        os.makedirs(output_dir, exist_ok=True)
        filepath = os.path.join(output_dir, filename)
        
        # Export without index
        df.to_csv(filepath, index=False, encoding='utf-8-sig')
        
        return filepath
    
    def export_to_parquet(self, df: pd.DataFrame, code: str, timeframe: str, 
                          start_date: datetime) -> str:
        """
        å¯¼å‡ºDataFrameä¸ºParquetæ ¼å¼ (v2.0)
        
        Args:
            df: è¦å¯¼å‡ºçš„DataFrame
            code: èµ„äº§ä»£ç 
            timeframe: æ—¶é—´ç²’åº¦
            start_date: å¼€å§‹æ—¥æœŸï¼ˆç”¨äºæ–‡ä»¶åï¼‰
        
        Returns:
            å¯¼å‡ºæ–‡ä»¶çš„å®Œæ•´è·¯å¾„
        """
        # ç”Ÿæˆæ–‡ä»¶åï¼ˆå¸¦æ—¥æœŸï¼‰
        start_str = start_date.strftime('%Y%m%d')
        filename = f"{code}_{timeframe}_{start_str}.parquet"
        
        # å¯¼å‡ºç›®å½•
        output_dir = "exported_data"
        os.makedirs(output_dir, exist_ok=True)
        filepath = os.path.join(output_dir, filename)
        
        # ç¡®ä¿Dateåˆ—ä¸ºdatetimeç±»å‹ï¼ˆParquetè¦æ±‚ï¼‰
        df_export = df.copy()
        if df_export['Date'].dtype == 'object':
            df_export['Date'] = pd.to_datetime(df_export['Date'])
        
        # å¯¼å‡º
        df_export.to_parquet(
            filepath,
            engine='pyarrow',
            compression='snappy',  # å‹ç¼©ç®—æ³•
            index=False
        )
        
        print(f"[DEBUG] Parquet exported to: {filepath}")
        return filepath
