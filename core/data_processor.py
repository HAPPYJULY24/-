"""
æ•°æ®å¤„ç†æ¨¡å— - ç”¨äºå¯¹é½å’Œåˆå¹¶å¤šä¸ªæœŸè´§å“ç§çš„æ•°æ®

ä¸»è¦åŠŸèƒ½ï¼š
1. æ•°æ®é‡é‡‡æ · (ç¡®ä¿ç»Ÿä¸€æ—¶é—´ç²’åº¦)
2. æ•°æ®å¯¹é½ä¸åˆå¹¶ (å¤„ç†ä¸åŒäº¤æ˜“æ—¶é—´)
3. ç”Ÿæˆ Ready-to-Use æ•°æ®é›†ç”¨äºå›æµ‹
"""

import os
import pandas as pd
from pathlib import Path
from datetime import datetime
from typing import Optional, Tuple


class DataProcessor:
    """æ•°æ®å¤„ç†å™¨ - è´Ÿè´£å¤šå“ç§æ•°æ®çš„å¯¹é½ä¸åˆå¹¶"""
    
    def __init__(self, store_dir: str = "data/store", output_dir: str = "data/processed"):
        """
        åˆå§‹åŒ–æ•°æ®å¤„ç†å™¨
        
        Args:
            store_dir: åŸå§‹æ•°æ®å­˜å‚¨ç›®å½•
            output_dir: å¤„ç†åæ•°æ®è¾“å‡ºç›®å½•
        """
        self.store_dir = Path(store_dir)
        self.output_dir = Path(output_dir)
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"[DataProcessor] åˆå§‹åŒ–å®Œæˆ")
        print(f"[DataProcessor] æ•°æ®æºç›®å½•: {self.store_dir}")
        print(f"[DataProcessor] è¾“å‡ºç›®å½•: {self.output_dir}")
    
    def align_datasets(
        self, 
        base_symbol: str = 'FCPO1!', 
        target_symbol: str = 'ZL1!',
        timeframe: str = '15m',
        output_filename: Optional[str] = None
    ) -> pd.DataFrame:
        """
        å¯¹é½å¹¶åˆå¹¶ä¸¤ä¸ªæœŸè´§å“ç§çš„æ•°æ®
        
        Args:
            base_symbol: åŸºå‡†å“ç§ä»£ç  (å¦‚ FCPO1!)
            target_symbol: ç›®æ ‡å“ç§ä»£ç  (å¦‚ ZL1!)
            timeframe: æ—¶é—´ç²’åº¦ (å¦‚ 15m)
            output_filename: è¾“å‡ºæ–‡ä»¶å (é»˜è®¤è‡ªåŠ¨ç”Ÿæˆ)
        
        Returns:
            åˆå¹¶åçš„ DataFrame
        
        Raises:
            FileNotFoundError: å¦‚æœæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨
            ValueError: å¦‚æœæ•°æ®æ ¼å¼ä¸æ­£ç¡®
        """
        print(f"\n{'='*60}")
        print(f"[DataProcessor] å¼€å§‹æ•°æ®å¯¹é½å¤„ç†")
        print(f"[DataProcessor] åŸºå‡†å“ç§: {base_symbol}")
        print(f"[DataProcessor] ç›®æ ‡å“ç§: {target_symbol}")
        print(f"[DataProcessor] æ—¶é—´ç²’åº¦: {timeframe}")
        print(f"{'='*60}\n")
        
        # 1. è¯»å–æ•°æ®æ–‡ä»¶
        base_df = self._load_data(base_symbol, timeframe)
        target_df = self._load_data(target_symbol, timeframe)
        
        print(f"[DataProcessor] âœ… æ•°æ®åŠ è½½å®Œæˆ")
        print(f"  - {base_symbol}: {len(base_df)} è¡Œ, æ—¶é—´èŒƒå›´ {base_df['Date'].min()} ~ {base_df['Date'].max()}")
        print(f"  - {target_symbol}: {len(target_df)} è¡Œ, æ—¶é—´èŒƒå›´ {target_df['Date'].min()} ~ {target_df['Date'].max()}")
        
        # 2. é‡é‡‡æ · (ç¡®ä¿ç»Ÿä¸€æ—¶é—´ç²’åº¦)
        base_df = self._resample_data(base_df, timeframe, f"{base_symbol}_")
        target_df = self._resample_data(target_df, timeframe, f"{target_symbol}_")
        
        # 3. åˆå¹¶æ•°æ® (Outer Join ä¿ç•™æ‰€æœ‰æ—¶é—´ç‚¹)
        print(f"\n[DataProcessor] ğŸ“Š å¼€å§‹åˆå¹¶æ•°æ® (Outer Join)...")
        merged_df = pd.merge(
            base_df, 
            target_df, 
            left_index=True, 
            right_index=True, 
            how='outer',
            suffixes=('', '_drop')  # é¿å…åˆ—åå†²çª
        )
        
        # åˆ é™¤é‡å¤çš„ Date åˆ—ï¼ˆå¦‚æœæœ‰ï¼‰
        merged_df = merged_df[[col for col in merged_df.columns if not col.endswith('_drop')]]
        
        print(f"[DataProcessor] âœ… åˆå¹¶å®Œæˆ: {len(merged_df)} è¡Œ")
        
        # 4. æ·»åŠ  overlap æ ‡è®°åˆ—
        merged_df = self._add_overlap_flag(merged_df, base_symbol, target_symbol)
        
        # 5. é‡ç½®ç´¢å¼•ï¼Œç¡®ä¿ Date ä¸ºåˆ—
        merged_df.reset_index(inplace=True)
        if 'index' in merged_df.columns:
            merged_df.rename(columns={'index': 'Date'}, inplace=True)
        
        # 6. æ•°æ®ç»Ÿè®¡
        self._print_statistics(merged_df, base_symbol, target_symbol)
        
        # 7. ä¿å­˜æ–‡ä»¶
        if output_filename is None:
            output_filename = f"merged_{base_symbol.replace('!', '')}_{target_symbol.replace('!', '')}_{timeframe}.parquet"
        
        output_path = self.output_dir / output_filename
        merged_df.to_parquet(output_path, index=False)
        
        print(f"\n[DataProcessor] ğŸ’¾ æ•°æ®å·²ä¿å­˜: {output_path}")
        print(f"[DataProcessor] æ–‡ä»¶å¤§å°: {output_path.stat().st_size / 1024 / 1024:.2f} MB")
        print(f"\n{'='*60}")
        
        return merged_df
    
    def _load_data(self, symbol: str, timeframe: str) -> pd.DataFrame:
        """
        åŠ è½½ Parquet æ•°æ®æ–‡ä»¶
        
        Args:
            symbol: æœŸè´§ä»£ç 
            timeframe: æ—¶é—´ç²’åº¦
        
        Returns:
            DataFrame with Date column
        """
        # æ„é€ æ–‡ä»¶å (Master DB æ ¼å¼: symbol_timeframe.parquet)
        filename = f"{symbol}_{timeframe}.parquet"
        filepath = self.store_dir / filename
        
        if not filepath.exists():
            raise FileNotFoundError(
                f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {filepath}\n\n"
                f"è¯·å…ˆä¸‹è½½æ•°æ®ï¼š\n"
                f"1. åœ¨ä¸»ç•Œé¢é€‰æ‹© 'BursaæœŸè´§ (TV)'\n"
                f"2. è¾“å…¥ä»£ç : {symbol}\n"
                f"3. é€‰æ‹©æ—¶é—´ç²’åº¦: {timeframe}\n"
                f"4. ç‚¹å‡»ä¸‹è½½"
            )
        
        print(f"[DataProcessor] ğŸ“– è¯»å–æ–‡ä»¶: {filepath.name}")
        df = pd.read_parquet(filepath)
        
        # ç¡®ä¿ Date åˆ—å­˜åœ¨
        if 'Date' not in df.columns:
            if df.index.name == 'Date' or isinstance(df.index, pd.DatetimeIndex):
                df.reset_index(inplace=True)
                if 'index' in df.columns:
                    df.rename(columns={'index': 'Date'}, inplace=True)
            else:
                raise ValueError(f"æ•°æ®æ–‡ä»¶ç¼ºå°‘ 'Date' åˆ—: {filepath}")
        
        # ç¡®ä¿ Date ä¸º datetime ç±»å‹
        df['Date'] = pd.to_datetime(df['Date'])
        
        return df
    
    def _resample_data(self, df: pd.DataFrame, timeframe: str, prefix: str = "") -> pd.DataFrame:
        """
        é‡é‡‡æ ·æ•°æ®åˆ°æŒ‡å®šæ—¶é—´ç²’åº¦
        
        Args:
            df: åŸå§‹ DataFrame
            timeframe: ç›®æ ‡æ—¶é—´ç²’åº¦ (å¦‚ '15m', '1h', '1d')
            prefix: åˆ—åå‰ç¼€ (ç”¨äºåŒºåˆ†ä¸åŒå“ç§)
        
        Returns:
            é‡é‡‡æ ·åçš„ DataFrame (Date ä½œä¸º index)
        """
        print(f"[DataProcessor] ğŸ”„ é‡é‡‡æ ·æ•°æ®åˆ° {timeframe}...")
        
        # è®¾ç½® Date ä¸ºç´¢å¼•
        df_resampled = df.set_index('Date')
        
        # æ˜ å°„æ—¶é—´ç²’åº¦
        freq_map = {
            '1m': '1min',
            '5m': '5min',
            '15m': '15min',
            '30m': '30min',
            '1h': '1H',
            '4h': '4H',
            '1d': '1D',
            '1w': '1W',
            '1M': '1ME'  # Month end
        }
        
        freq = freq_map.get(timeframe, timeframe)
        
        # OHLCV é‡é‡‡æ ·è§„åˆ™
        agg_dict = {
            'Open': 'first',
            'High': 'max',
            'Low': 'min',
            'Close': 'last',
            'Volume': 'sum'
        }
        
        # åªä¿ç•™å­˜åœ¨çš„åˆ—
        agg_dict = {k: v for k, v in agg_dict.items() if k in df_resampled.columns}
        
        # æ‰§è¡Œé‡é‡‡æ ·
        df_resampled = df_resampled.resample(freq).agg(agg_dict)
        
       # åˆ é™¤å…¨ä¸º NaN çš„è¡Œ (æ²¡æœ‰æ•°æ®çš„æ—¶é—´æ®µ)
        df_resampled = df_resampled.dropna(how='all')
        
        # æ·»åŠ åˆ—åå‰ç¼€
        if prefix:
            df_resampled.columns = [f"{prefix}{col}" for col in df_resampled.columns]
        
        print(f"[DataProcessor]   â†’ é‡é‡‡æ ·å: {len(df_resampled)} è¡Œ")
        
        return df_resampled
    
    def _add_overlap_flag(self, df: pd.DataFrame, base_symbol: str, target_symbol: str) -> pd.DataFrame:
        """
        æ·»åŠ  overlap æ ‡è®°åˆ—ï¼Œæ ‡ç¤ºä¸¤ä¸ªå“ç§éƒ½æœ‰äº¤æ˜“çš„æ—¶é—´æ®µ
        
        Args:
            df: åˆå¹¶åçš„ DataFrame
            base_symbol: åŸºå‡†å“ç§ä»£ç 
            target_symbol: ç›®æ ‡å“ç§ä»£ç 
        
        Returns:
            æ·»åŠ äº† is_overlap åˆ—çš„ DataFrame
        """
        print(f"[DataProcessor] ğŸ·ï¸  æ·»åŠ  overlap æ ‡è®°...")
        
        # æ£€æŸ¥ä¸¤ä¸ªå“ç§çš„ Close åˆ—æ˜¯å¦éƒ½æœ‰æ•°æ®
        base_col = f"{base_symbol}_Close"
        target_col = f"{target_symbol}_Close"
        
        if base_col in df.columns and target_col in df.columns:
            df['is_overlap'] = df[base_col].notna() & df[target_col].notna()
            overlap_count = df['is_overlap'].sum()
            print(f"[DataProcessor]   â†’ é‡å æ—¶é—´æ®µ: {overlap_count} è¡Œ ({overlap_count/len(df)*100:.1f}%)")
        else:
            print(f"[DataProcessor]   âš ï¸  æœªæ‰¾åˆ° Close åˆ—ï¼Œè·³è¿‡ overlap æ ‡è®°")
            df['is_overlap'] = False
        
        return df
    
    def _print_statistics(self, df: pd.DataFrame, base_symbol: str, target_symbol: str):
        """æ‰“å°æ•°æ®ç»Ÿè®¡ä¿¡æ¯"""
        print(f"\n[DataProcessor] ğŸ“ˆ æ•°æ®ç»Ÿè®¡:")
        print(f"  - æ€»è¡Œæ•°: {len(df)}")
        print(f"  - æ—¶é—´èŒƒå›´: {df['Date'].min()} ~ {df['Date'].max()}")
        print(f"  - æ—¶é—´è·¨åº¦: {(df['Date'].max() - df['Date'].min()).days} å¤©")
        
        # è®¡ç®—å„å“ç§çš„æ•°æ®å®Œæ•´æ€§
        base_close_col = f"{base_symbol}_Close"
        target_close_col = f"{target_symbol}_Close"
        
        if base_close_col in df.columns:
            base_coverage = df[base_close_col].notna().sum() / len(df) * 100
            print(f"  - {base_symbol} æ•°æ®è¦†ç›–ç‡: {base_coverage:.1f}%")
        
        if target_close_col in df.columns:
            target_coverage = df[target_close_col].notna().sum() / len(df) * 100
            print(f"  - {target_symbol} æ•°æ®è¦†ç›–ç‡: {target_coverage:.1f}%")
        
        if 'is_overlap' in df.columns:
            overlap_pct = df['is_overlap'].sum() / len(df) * 100
            print(f"  - é‡å æ—¶é—´æ®µå æ¯”: {overlap_pct:.1f}%")
    
    # ========== ğŸ†• Generic Alignment Method for GUI ==========
    
    def align_custom_files(
        self,
        file_path_a: str,
        file_path_b: str,
        output_filename: Optional[str] = None,
        apply_ffill: bool = True,
        ffill_asset: str = 'B'  # 'A', 'B', or 'both'
    ) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """
        é€šç”¨æ–‡ä»¶å¯¹é½æ–¹æ³• - æ”¯æŒä»»æ„ä¸¤ä¸ª Parquet æ–‡ä»¶çš„å¯¹é½ (GUI ç‰ˆæœ¬)
        
        **Killer Fixes:**
        1. æ—¶åŒºå¤„ç†ï¼šè‡ªåŠ¨æ£€æµ‹å¹¶ç»Ÿä¸€è½¬æ¢ä¸º UTC
        2. åŠ¨æ€åˆ—åï¼šä»æ–‡ä»¶åæå– symbol å¹¶é‡å‘½ååˆ—
        3. å‰å‘å¡«å……ï¼šå¯é€‰çš„ ffill() å¤„ç†ä¸åŒäº¤æ˜“æ—¶é—´
        
        Args:
            file_path_a: Asset A æ–‡ä»¶è·¯å¾„ (Base)
            file_path_b: Asset B æ–‡ä»¶è·¯å¾„ (Reference)
            output_filename: è¾“å‡ºæ–‡ä»¶å (é»˜è®¤è‡ªåŠ¨ç”Ÿæˆ)
            apply_ffill: æ˜¯å¦åº”ç”¨å‰å‘å¡«å……
            ffill_asset: å¯¹å“ªä¸ªèµ„äº§åº”ç”¨å¡«å…… ('A', 'B', or 'both')
        
        Returns:
            Tuple[å®Œæ•´ DataFrame, é¢„è§ˆ DataFrame (å‰50+å50è¡Œ)]
        """
        print(f"\n{'='*70}")
        print(f"[DataProcessor] ğŸ”„ Generic Alignment - GUI Mode")
        print(f"{'='*70}")
        print(f"[Asset A (Base)]:      {Path(file_path_a).name}")
        print(f"[Asset B (Reference)]: {Path(file_path_b).name}")
        print(f"{'='*70}\n")
        
        # 1. æå– Symbol åç§°ä»æ–‡ä»¶å
        symbol_a = self._extract_symbol_from_filename(file_path_a)
        symbol_b = self._extract_symbol_from_filename(file_path_b)
        
        print(f"[DataProcessor] ğŸ“ æå–çš„ Symbol:")
        print(f"  - Asset A: {symbol_a}")
        print(f"  - Asset B: {symbol_b}\n")
        
        # 2. åŠ è½½æ•°æ®æ–‡ä»¶ (ç›´æ¥ä»è·¯å¾„)
        df_a = self._load_parquet_file(file_path_a, symbol_a)
        df_b = self._load_parquet_file(file_path_b, symbol_b)
        
        print(f"[DataProcessor] âœ… æ–‡ä»¶åŠ è½½å®Œæˆ")
        print(f"  - {symbol_a}: {len(df_a)} è¡Œ")
        print(f"  - {symbol_b}: {len(df_b)} è¡Œ\n")
        
        # 3. ğŸ”¥ Killer Fix 1: æ—¶åŒºå¤„ç†
        df_a = self._fix_timezone(df_a, symbol_a)
        df_b = self._fix_timezone(df_b, symbol_b)
        
        # 4. ğŸ”¥ Killer Fix 2: åŠ¨æ€åˆ—åé‡å‘½å
        df_a = self._rename_columns_with_prefix(df_a, symbol_a)
        df_b = self._rename_columns_with_prefix(df_b, symbol_b)
        
        # 5. åˆå¹¶æ•°æ® (Outer Join)
        print(f"[DataProcessor] ğŸ“Š åˆå¹¶æ•°æ® (Outer Join)...\n")
        
        # ä½¿ç”¨ concat è€Œä¸æ˜¯ mergeï¼Œå› ä¸º Date å·²ç»æ˜¯ index
        merged_df = pd.concat([df_a, df_b], axis=1, join='outer')
        
        print(f"[DataProcessor] âœ… åˆå¹¶å®Œæˆ: {len(merged_df)} è¡Œ\n")
        
        # 6. ğŸ”¥ Killer Fix 3: å‰å‘å¡«å…… (Forward Fill)
        if apply_ffill:
            merged_df = self._apply_forward_fill(merged_df, symbol_a, symbol_b, ffill_asset)
        
        # 7. æ·»åŠ  overlap æ ‡è®°
        merged_df = self._add_generic_overlap_flag(merged_df, symbol_a, symbol_b)
        
        # 8. é‡ç½®ç´¢å¼•ï¼Œå°† Date è½¬ä¸ºåˆ—
        merged_df.reset_index(inplace=True)
        if 'index' in merged_df.columns:
            merged_df.rename(columns={'index': 'Date'}, inplace=True)
        
        # 9. ç»Ÿè®¡ä¿¡æ¯
        self._print_generic_statistics(merged_df, symbol_a, symbol_b)
        
        # 10. ä¿å­˜æ–‡ä»¶
        if output_filename is None:
            output_filename = f"aligned_{symbol_a.replace('!', '')}_{symbol_b.replace('!', '')}.parquet"
        
        output_path = self.output_dir / output_filename
        merged_df.to_parquet(output_path, index=False)
        
        print(f"\n[DataProcessor] ğŸ’¾ æ•°æ®å·²ä¿å­˜: {output_path}")
        print(f"[DataProcessor] æ–‡ä»¶å¤§å°: {output_path.stat().st_size / 1024 / 1024:.2f} MB\n")
        print(f"{'='*70}\n")
        
        # 11. ç”Ÿæˆé¢„è§ˆ DataFrame (å‰50 + å50è¡Œ)
        preview_df = self._generate_preview(merged_df)
        
        return merged_df, preview_df
    
    def _extract_symbol_from_filename(self, filepath: str) -> str:
        """
        ä»æ–‡ä»¶åæå– Symbol
        ä¾‹å¦‚: FCPO1!_15m.parquet -> FCPO1!
        """
        filename = Path(filepath).stem  # å»æ‰æ‰©å±•å
        # å‡è®¾æ ¼å¼æ˜¯ {symbol}_{timeframe}
        parts = filename.rsplit('_', 1)  # ä»å³è¾¹åˆ†å‰²ä¸€æ¬¡
        return parts[0] if parts else filename
    
    def _load_parquet_file(self, filepath: str, symbol: str) -> pd.DataFrame:
        """
        åŠ è½½å•ä¸ª Parquet æ–‡ä»¶å¹¶è¿”å› DataFrame
        """
        filepath = Path(filepath)
        
        if not filepath.exists():
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
        
        print(f"[DataProcessor] ğŸ“– è¯»å–: {filepath.name}")
        df = pd.read_parquet(filepath)
        
        # ç¡®ä¿æœ‰ Date åˆ—æˆ–ç´¢å¼•
        if 'Date' not in df.columns:
            if df.index.name == 'Date' or isinstance(df.index, pd.DatetimeIndex):
                df.reset_index(inplace=True)
                if 'index' in df.columns:
                    df.rename(columns={'index': 'Date'}, inplace=True)
            else:
                raise ValueError(f"æ•°æ®æ–‡ä»¶ç¼ºå°‘ 'Date' åˆ—æˆ–ç´¢å¼•: {filepath}")
        
        # è®¾ç½® Date ä¸ºç´¢å¼•
        df['Date'] = pd.to_datetime(df['Date'])
        df.set_index('Date', inplace=True)
        
        return df
    
    def _fix_timezone(self, df: pd.DataFrame, symbol: str) -> pd.DataFrame:
        """
        ğŸ”¥ Killer Fix 1: æ—¶åŒºå¤„ç†
        
        æ£€æŸ¥ç´¢å¼•æ—¶åŒºï¼Œå¦‚æœæœ‰æ—¶åŒºåˆ™è½¬æ¢ä¸º UTCï¼Œå¦‚æœæ²¡æœ‰åˆ™å‘å‡ºè­¦å‘Š
        """
        print(f"[Timezone Fix] æ£€æŸ¥ {symbol} çš„æ—¶åŒº...")
        
        if df.index.tz is not None:
            # æœ‰æ—¶åŒº - è½¬æ¢ä¸º UTC
            original_tz = df.index.tz
            print(f"  âœ… æ£€æµ‹åˆ°æ—¶åŒº: {original_tz} â†’ è½¬æ¢ä¸º UTC")
            df.index = df.index.tz_convert('UTC')
        else:
            # æ²¡æœ‰æ—¶åŒº (naive datetime)
            print(f"  âš ï¸  è­¦å‘Š: {symbol} çš„æ—¶é—´æˆ³ä¸º naive (æ— æ—¶åŒº)")
            print(f"     å‡è®¾ä¸ºæœ¬åœ°æ—¶é—´ï¼Œä¸è¿›è¡Œæ—¶åŒºè½¬æ¢")
            print(f"     å»ºè®®: ç¡®ä¿æ‰€æœ‰æ•°æ®æºä½¿ç”¨ç»Ÿä¸€æ—¶åŒº\n")
        
        return df
    
    def _rename_columns_with_prefix(self, df: pd.DataFrame, symbol: str) -> pd.DataFrame:
        """
        ğŸ”¥ Killer Fix 2: åŠ¨æ€åˆ—åé‡å‘½å
        
        å°†æ ‡å‡†åˆ—å (Open, High, Low, Close, Volume) é‡å‘½åä¸º {symbol}_Open ç­‰
        """
        print(f"[Column Rename] ä¸º {symbol} æ·»åŠ å‰ç¼€...")
        
        rename_map = {}
        for col in df.columns:
            if col in ['Open', 'High', 'Low', 'Close', 'Volume']:
                rename_map[col] = f"{symbol}_{col}"
        
        df.rename(columns=rename_map, inplace=True)
        
        print(f"  âœ… é‡å‘½ååˆ—: {list(rename_map.values())}\n")
        
        return df
    
    def _apply_forward_fill(
        self, 
        df: pd.DataFrame, 
        symbol_a: str, 
        symbol_b: str, 
        ffill_asset: str
    ) -> pd.DataFrame:
        """
        ğŸ”¥ Killer Fix 3: å‰å‘å¡«å…… (Forward Fill)
        
        å¯¹æŒ‡å®šèµ„äº§çš„åˆ—åº”ç”¨ ffill() ä»¥å¡«è¡¥äº¤æ˜“æ—¶é—´å·®å¼‚
        """
        print(f"[Forward Fill] åº”ç”¨å‰å‘å¡«å…… (asset: {ffill_asset})...")
        
        if ffill_asset == 'A' or ffill_asset == 'both':
            cols_a = [col for col in df.columns if col.startswith(f"{symbol_a}_")]
            if cols_a:
                df[cols_a] = df[cols_a].ffill()
                print(f"  âœ… å¡«å…… Asset A ({symbol_a}): {len(cols_a)} åˆ—")
        
        if ffill_asset == 'B' or ffill_asset == 'both':
            cols_b = [col for col in df.columns if col.startswith(f"{symbol_b}_")]
            if cols_b:
                df[cols_b] = df[cols_b].ffill()
                print(f"  âœ… å¡«å…… Asset B ({symbol_b}): {len(cols_b)} åˆ—")
        
        print()
        return df
    
    def _add_generic_overlap_flag(
        self, 
        df: pd.DataFrame, 
        symbol_a: str, 
        symbol_b: str
    ) -> pd.DataFrame:
        """æ·»åŠ  overlap æ ‡è®°åˆ— (é€šç”¨ç‰ˆæœ¬)"""
        close_a = f"{symbol_a}_Close"
        close_b = f"{symbol_b}_Close"
        
        if close_a in df.columns and close_b in df.columns:
            df['is_overlap'] = df[close_a].notna() & df[close_b].notna()
            overlap_count = df['is_overlap'].sum()
            print(f"[Overlap] é‡å æ—¶é—´æ®µ: {overlap_count} / {len(df)} ({overlap_count/len(df)*100:.1f}%)\n")
        else:
            df['is_overlap'] = False
        
        return df
    
    def _print_generic_statistics(
        self, 
        df: pd.DataFrame, 
        symbol_a: str, 
        symbol_b: str
    ):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯ (é€šç”¨ç‰ˆæœ¬)"""
        print(f"[DataProcessor] ğŸ“ˆ æ•°æ®ç»Ÿè®¡:")
        print(f"  - æ€»è¡Œæ•°: {len(df)}")
        
        if 'Date' in df.columns:
            print(f"  - æ—¶é—´èŒƒå›´: {df['Date'].min()} ~ {df['Date'].max()}")
            print(f"  - æ—¶é—´è·¨åº¦: {(df['Date'].max() - df['Date'].min()).days} å¤©")
        
        # è®¡ç®—è¦†ç›–ç‡
        close_a = f"{symbol_a}_Close"
        close_b = f"{symbol_b}_Close"
        
        if close_a in df.columns:
            coverage_a = df[close_a].notna().sum() / len(df) * 100
            print(f"  - {symbol_a} è¦†ç›–ç‡: {coverage_a:.1f}%")
        
        if close_b in df.columns:
            coverage_b = df[close_b].notna().sum() / len(df) * 100
            print(f"  - {symbol_b} è¦†ç›–ç‡: {coverage_b:.1f}%")
        
        if 'is_overlap' in df.columns:
            overlap_pct = df['is_overlap'].sum() / len(df) * 100
            print(f"  - é‡å æ—¶é—´æ®µ: {overlap_pct:.1f}%")
    
    def _generate_preview(self, df: pd.DataFrame, n_head: int = 50, n_tail: int = 50) -> pd.DataFrame:
        """
        ç”Ÿæˆé¢„è§ˆ DataFrame (å‰ n_head è¡Œ + å n_tail è¡Œ)
        
        ç”¨äº GUI æ˜¾ç¤ºï¼Œé¿å…åŠ è½½æ•´ä¸ªå¤§æ•°æ®é›†
        """
        print(f"\n[Preview] ç”Ÿæˆé¢„è§ˆæ•°æ® (å‰{n_head} + å{n_tail}è¡Œ)...")
        
        if len(df) <= (n_head + n_tail):
            # æ•°æ®é‡å°ï¼Œè¿”å›å…¨éƒ¨
            preview_df = df.copy()
        else:
            # æ‹¼æ¥å¤´å°¾
            head = df.head(n_head).copy()
            tail = df.tail(n_tail).copy()
            preview_df = pd.concat([head, tail])
        
        print(f"  âœ… é¢„è§ˆæ•°æ®: {len(preview_df)} è¡Œ\n")
        
        return preview_df

